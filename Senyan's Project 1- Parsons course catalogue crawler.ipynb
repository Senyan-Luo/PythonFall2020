{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why I made this\n",
    "For the first project, my learning goal is to practice crawling websites. \n",
    "Initially, I crawled Indeed.com for jobs and related information, but I hit a snag half way into the project when my bot failed CAPTCHA enforced by the website. After looking to Robots.txt on Indeed.com, I realized that it prohibits me crawling information in large quantity. \n",
    "\n",
    "I found it interesting that (almost) every site I visit has specific rules in regard to crawling. Parsons course catalogue is the first I encountered that allows all crawling behaviors. This is the primary reason why I pivoted from crawling indeed.com to Parson's course catalogue. Another reason is because I want to aggreagte course information in a way that conforms to my habit better:\n",
    "\n",
    "First, I wish to get more accurate results when I search by instructor name. If I search for Sven Travis on the current website, it would return all courses taught by instructors whose full name contains either \"Sven\" or \"Travis\", instead of just whom I am looking for (The best Sven there is.) \n",
    "Second, I want to see more information on the instructor for every course. I would usually look into the intructor if I am interested in any class, it would be great if I can see this information at the same place as the course. Since ratemyprofessors.com is heavily biased and prohibits crawling, I looked for course evals on Parsons. My efforts were to no avail because such information was only available to faculty. Luckily, I discovered that every faculty has a page in Parson's faculty directory, and the URL follows the same pattern that can be uniquely identified by the instructor name. \n",
    "Third, I wish to shorten the distance to get CRN for every class. On the course website, crn is not shown until I click open a specific course. Since this information comes handy for registration, I want to see it right away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "import csv\n",
    "import time \n",
    "from datetime import datetime \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why selenium library\n",
    "One of the biggest challenge I encoutered was dealing with the static url on Parsons course catalogue. To filter the courses that I can choose from, I would need to click on the filters to reveal those courses. However, the URL remains the same. This makes crawling tricky because requests relies on the URL to get the web page. \n",
    "\n",
    "To address this, I found the Selenium library. It enabled me to emulate a robot that can perform clicking and scrolling actions like a human user on a website. This way, even though the url remains static, the page I am crawling will contain the right information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\frech\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\frech\\anaconda3\\lib\\site-packages (from selenium) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\frech\\\\Anaconda3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#find my current root directory, program will look for some sort of web driver .exe there. \n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://courses.newschool.edu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automate clicking the fields to narrow down class selection\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  emulate a human user and click on specific filters on the course catalogue page. \n",
    "#  Therer are 3 filters I would click on while browsing classes: \"Art, Media, and Technolgy\", \"Graduate\" and \"Spring 2o21\"\n",
    "time.sleep(3)\n",
    "select = Select(driver.find_element_by_name('college[]'))\n",
    "select.select_by_value('art_media_technology')\n",
    "#driver.find_element_by_id(\"submit\").click()\n",
    "#print(driver.find_element_by_id(\"gradlevel001\").is_selected())\n",
    "level = driver.find_element_by_id('gradlevel001')\n",
    "driver.execute_script(\"arguments[0].click();\", level)\n",
    "term = driver.find_element_by_id('term000')\n",
    "driver.execute_script(\"arguments[0].click();\", term)\n",
    "#time.sleep delays execution of the next block. Much needed here because it takes a while for the page to actually update after bot selects fields. \n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, we got all the course links.\n"
     ]
    }
   ],
   "source": [
    "# get all the course urls from the inital page\n",
    "allCourseLinks = driver.find_elements_by_css_selector(\"div.crse_page p a\")\n",
    "links = []\n",
    "for link in allCourseLinks:\n",
    "    url = link.get_attribute(\"href\")\n",
    "    links.append(url)\n",
    "print(\"Done, we got all the course links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGFA 5000\n",
      "Andrea Geyer\n",
      "Mira Schor\n",
      "Shane Aslan Selzer\n",
      "Kamrooz Aram\n",
      "Jessica Rankin\n",
      "Faculty TBA\n",
      "Jennifer Woolfalk\n",
      "Simone Douglas\n",
      "Shoshana Dentz\n",
      "Peter Rostovsky\n",
      "Ester Partegas\n",
      "PGFA 5005\n",
      "Faculty TBA\n",
      "Faculty TBA\n",
      "Faculty TBA\n",
      "PGFA 5020\n",
      "Andrea Geyer and Lydia Matthews\n",
      "Andrea Geyer and Lydia Matthews\n",
      "PGFA 5050\n",
      "Faculty TBA\n",
      "Lydia Matthews\n",
      "Lenore Malen\n",
      "Neil Goldberg\n",
      "Faculty TBA\n",
      "PGFA 5051\n",
      "Faculty TBA\n",
      "Lydia Matthews\n",
      "Lenore Malen\n",
      "Neil Goldberg\n",
      "Lydia Matthews\n",
      "Shane Aslan Selzer\n",
      "LJ Roberts\n",
      "Faculty TBA\n",
      "PGFA 5127\n",
      "Andrea Geyer\n",
      "PGFA 5145\n",
      "Mira Schor\n",
      "PGFA 5151\n",
      "Sharmistha Ray\n",
      "PGFA 5300\n",
      "Phoenix Lindsey-Hall\n",
      "PGFA 5301\n",
      "Sammy Cucher\n",
      "Carrie Hawks\n",
      "PGFA 5302\n",
      "Sara Jimenez\n",
      "Andrea Geyer\n",
      "PGFA 5303\n",
      "Yve Laris Cohen\n",
      "PGFA 5900\n",
      "Kevin Bukreev\n",
      "Kevin Bukreev\n",
      "Kevin Bukreev\n",
      "PGPH 5001\n",
      "William Lamson\n",
      "William Lamson and MarieVic Vic\n",
      "Sandra Erbacher and William Lamson\n",
      "PGPH 5006\n",
      "Simone Douglas and Arthur Ou\n",
      "PGPH 5101\n",
      "Mike Crane and Keisha Scarville\n",
      "Anthony Aziz and Keren Moscovitch\n",
      "PGPH 5113\n",
      "Laura Parnes and James Ramer\n",
      "PGPH 5302\n",
      "Stacy Miller and James Ramer\n",
      "PGPH 5901\n",
      "Kevin Bukreev\n",
      "Kevin Bukreev and Arthur Ou\n",
      "PGPH 5902\n",
      "Faculty TBA\n",
      "Faculty TBA\n",
      "James Ramer\n",
      "James Ramer\n",
      "PGTE 5200\n",
      "Faculty TBA\n",
      "Morry Galonoy\n",
      "Harpreet Sareen\n",
      "Kyle Li\n",
      "American Artist and Salome Asega\n",
      "Mohini Dutta\n",
      "Faculty TBA\n",
      "Lauren Slowik\n",
      "PGTE 5201\n",
      "Faculty TBA\n",
      "PGTE 5250\n",
      "Faculty TBA\n",
      "Lan Zhang\n",
      "Bolor Amgalan\n",
      "Faculty TBA\n",
      "Cassandra Hradil\n",
      "Elena Gold and Jonathan Packles\n",
      "Aarati Akkapeddi and Shirley Leung\n",
      "PGTE 5251\n",
      "Faculty TBA\n",
      "Katherine Moriwaki and Xin Xin\n",
      "PGTE 5301\n",
      "Faculty TBA\n",
      "PGTE 5500\n",
      "Jonathan Beilin\n",
      "Jonathan Beilin\n",
      "PGTE 5566\n",
      "John Bruneau\n",
      "Sven Travis\n",
      "Carmine Guida\n",
      "PGTE 5600\n",
      "Faculty TBA\n",
      "David Carroll and Mohini Dutta\n",
      "PGTE 5601\n",
      "Faculty TBA\n",
      "Justin Charles\n",
      "Jane Nishimura\n",
      "Angel Lopez\n",
      "Andrew Zornoza\n",
      "Jeanne Lambert\n",
      "PGTE 5901\n",
      "Kevin Bukreev\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a869df9af336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mcheckSeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallSections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'seats'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mcheckStatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallSections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mclassTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallSections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'days'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mallSections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'times'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mdateRange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallSections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'dates'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstructor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#iterate through the course links, use Beautiful Soup from now on, because no user interaction needs to happen on the \n",
    "# specific course page\n",
    "# save all the data into a dictionary. One course can have many sections, use course number as the keys. \n",
    "scrapeData = []\n",
    "\n",
    "for i in range(len(links)):\n",
    "    courseLink = links[i]\n",
    "    response = requests.get(courseLink)\n",
    "    soupObject = BeautifulSoup(response.text, \"html.parser\")\n",
    "    sectionInfoArr = []\n",
    "    #get the course ID, same for all sections\n",
    "    courseID = soupObject.find('p','dept').text + \" \" + soupObject.find('p','crse').text\n",
    "    print(courseID)\n",
    "\n",
    "    #get all sections under the same course ID\n",
    "    allSections = soupObject.find_all('div', 'section_details')\n",
    "    \n",
    "    #TODO, so far we are only getting 1 section from each course, how to get all? what data structure?\n",
    "    for j in range(len(allSections)):\n",
    "        sectionTitle = allSections[j].find('div','title').h1.text\n",
    "        instructor = allSections[j].find('div','instructor').text.partition(':')[2].strip()\n",
    "        crn = allSections[j].find('div','crn').text.partition(':')[2].strip()\n",
    "        description = allSections[j].find('div','description').text.strip()\n",
    "        checkSeats = allSections[j].find('div','seats').span.text\n",
    "        checkStatus = allSections[j].find('div','status').span.text\n",
    "        classTime = allSections[j].find('div','days').text.partition(':')[2].strip() + allSections[j].find('div','times').text.partition(':')[2]\n",
    "        dateRange = allSections[j].find('div','dates').text.partition(':')[2].strip()\n",
    "        print(instructor)\n",
    "    \n",
    "    # get every instructor's info page from https://www.newschool.edu/parsons/faculty/\n",
    "    # this link follows the same format, insert - between names of the instructor.\n",
    "    # TODO: if a course is co-taught by two or more professors, need to parse further\n",
    "        instructorDash = instructor.replace(' ', '-');\n",
    "        instructorInfo = '';\n",
    "        insUrl = 'https://www.newschool.edu/parsons/faculty/' + instructorDash\n",
    "        response = requests.get(insUrl)\n",
    "        time.sleep(1)\n",
    "    \n",
    "        if (response.status_code == 200):\n",
    "            instructorInfo = insUrl\n",
    "        else:\n",
    "            instructorInfo = \"Not available\"\n",
    "        \n",
    "        sectionInfo = {'course ID': courseID, 'section title': sectionTitle, 'CRN':crn, 'instructor':instructor, 'instructorInfo':instructorInfo, 'classTime':classTime, 'dateRange':dateRange, 'checkSeats':checkSeats, 'checkStatus':checkStatus, 'description':description, 'courseLink':courseLink}\n",
    "        #sectionInfoArr.append(sectionInfo)\n",
    "    #scrapeData[courseID] = sectionInfoArr\n",
    "    # make every class a dictionary:\n",
    "        scrapeData.append(sectionInfo)\n",
    "print(\"Done, all courses were scrapped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 60)\n",
    "    \n",
    "#'section title': sectionTitle, 'CRN':crn, 'instructor':instructor, 'instructorInfo':instructorInfo, 'classTime':classTime, 'dateRange':dateRange, 'checkSeats':checkSeats, 'checkStatus':checkStatus, 'description':description, 'courseLink':courseLink\n",
    "df = pd.DataFrame.from_dict(scrapeData)\n",
    "#df.style.set_properties(subset=['description'], **{'width': '10px'})\n",
    "#group by course ID. \n",
    "#df.groupby(df['course ID'])\n",
    "#df.set_index('course ID')\n",
    "\n",
    "df.columns = ['ID', 'Name', 'CRN', 'Instructor', 'Insturctor Website', 'Time', 'Duration', 'Open Seats', 'Status', 'Description', 'Course Link']\n",
    "df\n",
    "# output_file = 'SP21Courses.xlsx'\n",
    "# # saving the excel \n",
    "# df.to_excel(output_file) \n",
    "# print('DataFrame is written to Excel File successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for course by instructor:\n",
    "def searchByInstructor(name):\n",
    "    # take user input, format it so that first letter is capitalized. \n",
    "    print(name)\n",
    "    name_formatted = name.lower().title()\n",
    "    print(name_formatted)\n",
    "    taught_by = df[df['Instructor'].str.contains(name_formatted)]\n",
    "    display(taught_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searach by class ID\n",
    "def searchCourseID(id):\n",
    "    course_by_id = df[df['ID'] == id]\n",
    "    display(course_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for course by CRN\n",
    "def searchByCrn(crnNum):\n",
    "    course_by_crn = df[df['CRN'] == crnNum]\n",
    "    display(course_by_crn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt user for further action after every search. \n",
    "def searchAgain():\n",
    "    userInput = input('''Do you wish to search again? y for Yes and n for No''')\n",
    "    if userInput == 'y':\n",
    "        search()\n",
    "    elif userInput == 'n':\n",
    "        print('Coolio. Have a great semester!')\n",
    "    else:\n",
    "        print('Please enter y or n')\n",
    "        searchAgain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    search_by = input('''How do you wish to search for a class:\n",
    "a. by instructor   b. by CRN   c. by course ID    ''')\n",
    "    if search_by == 'a' or 'A':\n",
    "        instructor_name = input('What is the full name of the instructor? i.e. Sven Travis    ')\n",
    "        searchByInstructor(instructor_name)   \n",
    "    elif search_by == 'b' or 'B':\n",
    "        print(\"I chose b!!!!!\")\n",
    "        crn = input('What is CRN number for the course? i.e. 3011    ')\n",
    "        searchByCrn(crn)\n",
    "    elif search_by == 'c' or 'C':\n",
    "        course_id = input('What is couse ID? i.e. PGTE 5300    ')\n",
    "        searchCourseID(course_id)\n",
    "    else:\n",
    "        print(\"Sorry Please enter a valid string\")   \n",
    "    searchAgain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
